---
title: "Estimating Truth by Repetition: A Monte Carlo Simulation"
author: "Group 3: Andi Mellyn, Jessica Reyes, Hope Winsor"
format: revealjs
editor: visual
---

## Sample Size for Binary Logistic Prediction Models: Beyond events per variable criteria

-   clinical simulation studies for binary outcomes

    -   want accurate predictions on new data

-   *Events Per Variable* (EPV), the ratio of the number of events in the smaller class to the number of degrees of freedom

    -   think signal to noise

    -   minimum of 10 is standard

## Research Questions

-   What effect does EPV have on out-of-sample prediction for logistic regression?

-   Are there metrics that better estimate out-of-sample prediction? If so, which ones?

-   Beyond the results of the paper: What effect does EPV have on out-of-sample prediction for non-parametric methods of binary classification?

## Simulation Study

-   created more than 4,000 different combinations and over 12,000,000 datasets

    -   different combinations of EPV, event ratios, number and distribution of predictors, AUC, and predictor effects

-   7 different models

    -   Maximum Likelihood, backwards subset selection, Firth's penalized likelihood, heuristic shrinkage, Ridge, LASSO

## Our Implementation

-   Variables for consideration

    -   EPV: 5, 10, 50

    -   Number of predictors: 4, 8, 12

    -   Distribution of predictors: MVN(0), MVN(0.5)

    -   Predictor effects: all equal, one strong, one noise, half noise

-   Predictors normally distributed

-   Training data set has $N = \frac{EPV * P}{Pr(Y = 1)}$ data points

-   Testing dataset has $N^* = \frac{5000}{Pr(Y = 1)}$ data points

## Maximum Likelihood

```{r}
#| echo: false
library(MASS)
library(pROC)
library(dplyr)
library(glmnet)
library(tibble)
library(ggplot2)
```

```{r}
#| echo: false
# Data Simulation Function

data_simulation_function <- function(N, P, rho, beta_pattern) {
  Sigma <- matrix(rho, P, P)
  diag(Sigma) = 1
  
  X <- mvrnorm(n = N, mu = rep(0, P), Sigma = Sigma)
  
  # Set coefficients
  if (beta_pattern == "equal") {
    beta <- rep(0.5, P)
  } else if (beta_pattern == "strong") {
    beta <- c(1, rep(0.2, P-1))
  } else if (beta_pattern == "noise") {
    beta <- c(0, rep(0.3, P-1))
  } else if (beta_pattern == "halfnoise") {
    beta <- c(rep(0, P/2), rep(0.3, P/2))
  } else stop("Unknown beta pattern")
  
  eta <- X %*% beta
  p <- 1 / (1 + exp(-eta))
  
  y <- rbinom(N, 1, p)
  
  data.frame(y = y, X)
}
```

```{r}
#| echo: false
# Model Fitting

model_fitting_function <- function(train_data) {
  glm(y ~ ., data = train_data, family = binomial)
}

```

```{r}
#| echo: false
# Model Assessment function
model_assessment_function <- function(model, test_data) {
  probs <- predict(model, newdata = test_data, type = "response")
  
  auc_val <- auc(test_data$y, probs)
  brier <- mean((test_data$y - probs)^2)
  
  tibble(
    AUC = as.numeric(auc_val),
    Brier = brier
  )
}
```

```{r}
#| echo: false
set.seed(400)
# Simulation loop

EPV_vals <- c(5, 10, 50)
event_frac <- 0.5    

P_vals <- c(4, 8, 12)
rho_vals <- c(0, 0.5)
beta_patterns <- c("equal", "strong", "noise", "halfnoise")

B <- 10  

results_list <- list()
iter <- 1

for (EPV in EPV_vals) {
  for (P in P_vals) {
    for (rho in rho_vals) {
      for (bp in beta_patterns) {
        for (b in 1:B) {
          
          # Determine sample sizes 
          N  <- ceiling((EPV * P) / event_frac)
          N_star <- ceiling(5000 / event_frac)   
          
          # Training data 
          train_data <- data_simulation_function(N, P, rho, bp)
          
          # Test data
          test_data  <- data_simulation_function(N_star, P, rho, bp)
          
          # fit model
          fit_model <- model_fitting_function(train_data)
          
          # evaluate model
          metrics <- model_assessment_function(fit_model, test_data)
          
          metrics$model <- "MLE"  
          
          # metadata
          metrics <- metrics %>%
            mutate(
              model = "MLE",
              EPV = EPV,
              P = P,
              rho = rho,
              beta_pattern = bp,
              sim = b
            )
          
          # results
          results_list[[iter]] <- metrics
          iter <- iter + 1
        }
      }
    }
  }
}
results_df <- dplyr::bind_rows(results_list); results_df
```

## ROC curve

The ROC curve shows that the logistic MLE model can tell the two outcome groups apart better than random guessing, because the curve lies above the diagonal.

```{r}
#| echo: false
roc_obj <- roc(train_data$y, predict(fit_model, type="response"))
plot(roc_obj, col="rosybrown", lwd=2)
```

## Calibration Plot

The calibration curve follows the diagonal closely, indicating good overall calibration, but it rises slightly above the predicted probabilities of about 0.25-0.50, meaning the model slightly underestimates the true event probability in that range.

```{r}
library(ggplot2)

cal_df <- train_data %>%
  mutate(prob = predict(fit_model, type="response"),
         decile = ntile(prob, 10)) %>%
  group_by(decile) %>%
  summarize(obs = mean(y), pred = mean(prob))

ggplot(cal_df, aes(pred, obs)) +
  geom_point(size = 3) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, color = "yellow4", lty = 2) +
  ylim(0,1) + xlim(0,1) +
  ggtitle("Calibration Plot")
```

## Ridge Logistic Regression

$Loss(w) = - l(w) + \lambda \sum_{j=1}^pw_j^2$

-   Is an L2 **penalized** logistic regression model that shrinks the coefficients towards zero

-   Cross-validation selects $\lambda$ automatically

-   It addresses instability when EPV is low

-   Leads to more stable predictions when predictors are correlated or noisy

## Ridge Monte Carlo Performance

:::: {.columns}
::: {.column width="45%"}
![](ridge_brier.png){width="85%"}

::: smallest
Brier score is lower when predictors are correlated (P = 0) showing how ridge is well suited to deal with this.
:::
:::

::: {.column width="45%"}
![](ridge_auc.png){width="85%"}

::: smallest
Discrimination improves with sample size and is strongest under predictor correlation (œÅ = 0.5)  
:::
:::
::::

## Ridge Calibration Plot

![](ridge_cali.png){width="100%"}

Ridge shows strong overall calibration. At low predicted probabilities it tends to overestimate & between ~0.5 & 0.75 it underestimates.
At higher probabilities, ridge shows almost perfect calibration.

## Random Forest

```{r}

```

## Maximum Likelihood vs. Ridge

```{r}
#| echo: false
library(MASS)
library(pROC)
library(dplyr)
library(glmnet)
library(tibble)
library(ggplot2)
```

```{r}
#| echo: false
simulate_logit <- function(N, P, rho, beta_pattern) {
  Sigma <- matrix(rho, P, P)
  diag(Sigma) <- 1
  
  X <- mvrnorm(n = N, mu = rep(0, P), Sigma = Sigma)
  
  if (beta_pattern == "equal") {
    beta <- rep(0.5, P)
  } else if (beta_pattern == "strong") {
    beta <- c(1.0, rep(0.2, P-1))
  } else if (beta_pattern == "noise") {
    beta <- c(0, rep(0.3, P-1))
  } else if (beta_pattern == "halfnoise") {
    beta <- c(rep(0, P/2), rep(0.3, P/2))
  } else {
    stop("Unknown beta pattern")
  }
  
  eta <- X %*% beta
  pi  <- 1/(1 + exp(-eta))
  y <- rbinom(N, 1, pi)
  
  data.frame(y = y, X)
}
```

```{r}
#| echo: false
fit_logistic_mle <- function(dat) {
  glm(y ~ ., data = dat, family = binomial)
}

evaluate_mle_model <- function(fit, dat) {
  probs <- predict(fit, type = "response")
  auc_val <- auc(dat$y, probs)
  brier <- mean((dat$y - probs)^2)
  
  tibble(AUC = as.numeric(auc_val), Brier = brier, lambda = NA)
}
```

```{r}
#| echo: false
fit_logistic_ridge <- function(dat) {
  X <- as.matrix(dat[, -1])
  y <- dat$y
  
  cv.glmnet(x = X, y = y, alpha = 0, family = "binomial")
}

evaluate_ridge_model <- function(cvfit, dat) {
  X <- as.matrix(dat[, -1])
  y <- dat$y
  
  probs <- predict(cvfit, newx = X, type = "response", s = "lambda.min")
  
  auc_val <- auc(y, probs)
  brier <- mean((y - probs)^2)
  
  tibble(
    AUC = as.numeric(auc_val),
    Brier = brier,
    lambda = cvfit$lambda.min
  )
}
```

```{r}
#| echo: false
set.seed(400)

EPV_vals <- c(5, 10, 50)
event_frac <- 0.5    # FIXED

P_vals <- c(4, 8, 12)
rho_vals <- c(0, 0.5)
beta_patterns <- c("equal", "strong", "noise", "halfnoise")

B <- 10  # number of Monte Carlo repetitions

all_results <- list()
iter <- 1

for (EPV in EPV_vals) {
  for (P in P_vals) {
    for (rho in rho_vals) {
      for (bp in beta_patterns) {
        for (b in 1:B) {
          
          # Determine total sample size
          N <- ceiling((EPV * P) / event_frac)
          
          # Simulate dataset
          dat <- simulate_logit(N, P, rho, bp)
          
          ### MLE ###
          fit_mle <- fit_logistic_mle(dat)
          eval_mle <- evaluate_mle_model(fit_mle, dat)
          eval_mle$model <- "MLE"
          
          ### Ridge ###
          fit_ridge <- fit_logistic_ridge(dat)
          eval_ridge <- evaluate_ridge_model(fit_ridge, dat)
          eval_ridge$model <- "Ridge"
          
          ### Store combined ###
          combined <- bind_rows(eval_mle, eval_ridge) %>%
            mutate(
              EPV = EPV,
              P = P,
              rho = rho,
              beta_pattern = bp,
              sim = b
            )
          
          all_results[[iter]] <- combined
          iter <- iter + 1
        }
      }
    }
  }
}

results_df <- bind_rows(all_results)
```

```{r}
#| echo: false
summary_df <- results_df %>%
  group_by(model, EPV, P, rho, beta_pattern) %>%
  summarise(
    mean_AUC = mean(AUC),
    sd_AUC = sd(AUC),
    mean_Brier = mean(Brier),
    sd_Brier = sd(Brier),
    mean_lambda = mean(lambda, na.rm = TRUE),
    .groups = "drop"
  )

knitr::kable(summary_df, digits = 3)
```

```{r}
#| echo: false
ggplot(summary_df,
       aes(x = factor(P),
           y = mean_AUC,
           color = factor(rho),
           group = factor(rho))) +
  geom_point(size = 2.8) +
  geom_line(linewidth = 0.8) +
  geom_ribbon(aes(ymin = mean_AUC - sd_AUC,
                  ymax = mean_AUC + sd_AUC,
                  fill = factor(rho)),
              alpha = 0.25,
              color = NA) +
  facet_grid(model ~ beta_pattern) +
  labs(
    title = "AUC: MLE vs Ridge",
    x = "Number of Predictors (P)",
    y = "Mean AUC",
    color = "Correlation (rho)",
    fill  = "Correlation (rho)"
  ) +
  theme_minimal(base_size = 14)
```

## Main Findings

1.  When EPV is small (EPV=5), Ridge performs better overall than MLE, showing:

-   higher or similar AUC
-   lower Brier scores
-   much lower variance
-   more realistic estimates when predictors are correlated

2.  At EPV=10, both methods perform comparably,

-   Although, Ridge remains slightly more stable, especially in the presence of noisy predictors.

3.  At EPV=50, MLE and Ridge are almost identical, because shrinkage is no longer needed.

4.  Ridge's adaptive penalty $(\lambda)$ adjusts appropriately:

-   strong shrinkage at low EPV
-   minimal shrinkage at high EPV
-   $\rightarrow$ Ridge regression is advantageous when EPV is low or when predictors contain noise or are correlated. When EPV is sufficiently high $(\ge50)$, both methods perform equivalently.

```{r}

```

## Random Forest vs. MLE & Ridge

```{r}

```

## Conclusion

::: smallest
-   Across all models, EVP alone did not explain predictive performance
-   Ridge & Random Forest often outperformed MLE at low EPV, but performance improved with larger N and correlated predictors, not EPV
-   Calibration patterns showed shrinkage effects: Ridge slightly underestimates mid-range probabilities
-   Overall, our Monte Carlo results reinforce that **sample size, predictor structure, & signal strength** matter more than EPV when evaluating logistic prediction models
:::
