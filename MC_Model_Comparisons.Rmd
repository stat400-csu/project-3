---
title: "MC Model Comparisons"
author: "Group 3"
date: "2025-12-01"
output:
  pdf_document: default
  html_document: default
---
\colorlet{shadecolor}{gray!10}
```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(MASS)
library(GGally)
library(tidymodels)
library(glmnet)
library(pROC)
library(randomForest)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy = TRUE)
```

\newcommand{\hstart}{ \colorlet{shadecolor}{green!20}
\begin{shaded} }
\newcommand{\hstop}{  \end{shaded} \colorlet{shadecolor}{gray!10}}

\hstart

# Maximum Liklihood, Ridge, and Random Forest: A Monte Carlo Model Comparison

\hstop

\hstart

### 0. Source functions and simulated data for model comparisons

\hstop

```{r source functions}
# sourcing method functions

source("simulate_functions.R")  # for 

```

```{r simulated data}
# sourcing simulated data

simulated_data <- readRDS("data/simulated_data.rds")

```

\hstart

### 1. Background
replicating "Sample size for binary logistic prediction models: Beyond events per variable criteria" by Smeden et al.

out-of-sample performance of developed prediction models studied before and after regression shrinkage and variable selection

concluded EPV does not have a strong relation with metrics of predictive performance and is not an appropriate cirterian for binary prediction model development studies

better approximated by considering number of predictors, total sample size, and events fraction

key contributing factor to predictive performance is size of data set relative to number of predictors

events per variable: ratio of number of events, the number of observations in smaller of two outcome groups, relative to the number of degrees of freedom/parameters required to represent the predictors considered in the model

medical literature uses EPV of 10 as minimum but there is skepticism as to the validity of the scientific backing of this benchmark. in step-wise predictor selection, EPV greater than 50 may be needed. Good out-of-sample predictive performance demonstrated with regression shrinkage techniques with EPV less than 10.

modeling:
predictive performance evaluated on large sample validation data sets

maximum likelihood (ML), ridge regression, LASSO, Firth's correction, heuristic shrinkage after ML estimation, backwards elimination predictor selection

EPV, the events fraction, number of candidate predictors, area under the ROC curve, distribution of predictor variables, type of predictor variable effects

factorial simulation study with 4,032 models
5,000 simulation runs

1. development data set generated
    a. for each of N = EPV * P/Pr(Y = 1) individuals, draw predictor variable vector $\mathbf{x}_i$
    b. for each individual, generate $y_i$ = Bernoulli($\pi_i$)
2. fit binary logistic prediction models with each of the considered regression shrinkage and predictor selection strategies
3. large validation data set generated, N* = 5000/Pr(Y = 1), using sampling approach from (1)
4. evaluate performance of prediction models (2) on validation data (3)

measures of performance:
- average loss of area under ROC curve: average difference between AUC of generated data and AUC of model (defined by distribution of predictor variables); higher values closer to zero indicate better discriminative performance
- median calibration slope (CS): closer to 1 = better performance, cs < 1 = overfitting, cs > 1 = underfitting
- average calibration in the large (CIL): difference between generated evens fraction and average estimated probabilities $\frac{\sum_{i} y_i}{N} - \frac{\sum_i \hat{\pi}_i}{N}$; closer to 0 = better performance, CIL > 0 = systematic overestimation of estimated probabilities, CIL < 0 = underestimation
- average Brier score
- square root of mean squared prediction error
- mean absolute prediction error

issues with maximum likelihood:

1. not optimal for making model predictions of the expected risk in new individuals -- in most circumstances shrinkage estimators can be defined that have lower expected error for estimating probabilities in new individuals than ML estimators; benefit of shrinkage decreases with increasing EPV
2. ML coefficients are biased toward more extreme effects; bias reduces with increasing EPV but may not completely disappear
3. model estimation becomes unstable when predictor effects are large or sparse -- results in extreme probability estimates close to boundaries of 0 or 1; less likely with increasing EPV
4. estimation becomes unstable when predictors are strongly correlated, inflates standard errors for predictor effects; spurious collinearity less likely with increasing EPV

Ridge regression:
penalizes the likelihood proportionally to the sum of squared predictor effects

predictors standardized to have mean zero and unit variance

10-fold cross validation used to determine the optimal value for the tuning parameter

used to deal with collinearity (4), can also be used to deal with separation (3)

GLMnet used to estimate Ridge, logistf used to estimate ML, MASS used to generate predictor data

\hstop

\hstart

=======
results:
- improved predictive performance when EPV increased
- little association between Brier and EPV beyond EPV 20
- CIL consistently close to perfect average except for Firth
- CS improved with increasing EPV
- all models except for Ridge showed signs of overfitting (CS above 1)
- impact of shrinkage lessened with increasing EPV
- little variation in Brier and CIL between shrinkage strategies

### 2. Maximum Liklihood

\hstop

```{r maximum likelihood echo=FALSE}
# Maximum Likelihood
library(MASS)
library(pROC)
library(dplyr)
library(glmnet)
library(tibble)
library(ggplot2)
```

```{r echo=FALSE, results='hide', include=FALSE, message=FALSE}
# Data Simulation Function

data_simulation_function <- function(N, P, rho, beta_pattern) {
  Sigma <- matrix(rho, P, P)
  diag(Sigma) = 1
  
  X <- mvrnorm(n = N, mu = rep(0, P), Sigma = Sigma)
  
  # Set coefficients
  if (beta_pattern == "equal") {
    beta <- rep(0.5, P)
  } else if (beta_pattern == "strong") {
    beta <- c(1, rep(0.2, P-1))
  } else if (beta_pattern == "noise") {
    beta <- c(0, rep(0.3, P-1))
  } else if (beta_pattern == "halfnoise") {
    beta <- c(rep(0, P/2), rep(0.3, P/2))
  } else stop("Unknown beta pattern")
  
  eta <- X %*% beta
  p <- 1 / (1 + exp(-eta))
  
  y <- rbinom(N, 1, p)
  
  data.frame(y = y, X)
}
```

```{r echo=FALSE, results='hide', include=FALSE, message=FALSE}
# Model Fitting

model_fitting_function <- function(train_data) {
  glm(y ~ ., data = train_data, family = binomial)
}

```

```{r echo=FALSE, results='hide', include=FALSE, message=FALSE}
# Model Assessment function
model_assessment_function <- function(model, test_data) {
  probs <- predict(model, newdata = test_data, type = "response")
  
  auc_val <- auc(test_data$y, probs)
  brier <- mean((test_data$y - probs)^2)
  
  tibble(
    AUC = as.numeric(auc_val),
    Brier = brier
  )
}

```

```{r}
set.seed(400)
# Simulation loop

EPV_vals <- c(5, 10, 50)
event_frac <- 0.5    

P_vals <- c(4, 8, 12)
rho_vals <- c(0, 0.5)
beta_patterns <- c("equal", "strong", "noise", "halfnoise")

B <- 10  

results_list <- list()
iter <- 1

for (EPV in EPV_vals) {
  for (P in P_vals) {
    for (rho in rho_vals) {
      for (bp in beta_patterns) {
        for (b in 1:B) {
          
          # Determine sample sizes 
          N  <- ceiling((EPV * P) / event_frac)
          N_star <- ceiling(5000 / event_frac)   
          
          # Training data 
          train_data <- data_simulation_function(N, P, rho, bp)
          
          # Test data
          test_data  <- data_simulation_function(N_star, P, rho, bp)
          
          # fit model
          fit_model <- model_fitting_function(train_data)
          
          # evaluate model
          metrics <- model_assessment_function(fit_model, test_data)
          
          metrics$model <- "MLE"  
          
          # metadata
          metrics <- metrics %>%
            mutate(
              model = "MLE",
              EPV = EPV,
              P = P,
              rho = rho,
              beta_pattern = bp,
              sim = b
            )
          
          # results
          results_list[[iter]] <- metrics
          iter <- iter + 1
        }
      }
    }
  }
}
results_df <- dplyr::bind_rows(results_list); results_df
```

```{r}
roc_obj <- roc(train_data$y, predict(fit_model, type="response"))
plot(roc_obj, col="rosybrown", lwd=2)
```

```{r}
library(ggplot2)

cal_df <- train_data %>%
  mutate(prob = predict(fit_model, type="response"),
         decile = ntile(prob, 10)) %>%
  group_by(decile) %>%
  summarize(obs = mean(y), pred = mean(prob))

ggplot(cal_df, aes(pred, obs)) +
  geom_point(size = 3) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, color = "yellow4", lty = 2) +
  ylim(0,1) + xlim(0,1) +
  ggtitle("Calibration Plot")
```

\hstart

### 3. Ridge Regression

\hstop

```{r ridge regression}
# ridge regression


```

\hstart

### 4. Random Forest

\hstop
```{r random forest}
# random forest


```


\hstart

### 5. Comparisons

\hstop

```{r comparisons}
# comparisons


```

```{r maximum likelihood vs Ridge}
# Maximum Likelihood
library(MASS)
library(pROC)
library(dplyr)
library(glmnet)
library(tibble)
library(ggplot2)

```

```{r echo=FALSE, results='hide', include=FALSE, message=FALSE}
simulate_logit <- function(N, P, rho, beta_pattern) {
  Sigma <- matrix(rho, P, P)
  diag(Sigma) <- 1
  
  X <- mvrnorm(n = N, mu = rep(0, P), Sigma = Sigma)
  
  if (beta_pattern == "equal") {
    beta <- rep(0.5, P)
  } else if (beta_pattern == "strong") {
    beta <- c(1.0, rep(0.2, P-1))
  } else if (beta_pattern == "noise") {
    beta <- c(0, rep(0.3, P-1))
  } else if (beta_pattern == "halfnoise") {
    beta <- c(rep(0, P/2), rep(0.3, P/2))
  } else {
    stop("Unknown beta pattern")
  }
  
  eta <- X %*% beta
  pi  <- 1/(1 + exp(-eta))
  y <- rbinom(N, 1, pi)
  
  data.frame(y = y, X)
}
```

```{r echo=FALSE, results='hide', include=FALSE, message=FALSE}
fit_logistic_mle <- function(dat) {
  glm(y ~ ., data = dat, family = binomial)
}

evaluate_mle_model <- function(fit, dat) {
  probs <- predict(fit, type = "response")
  auc_val <- auc(dat$y, probs)
  brier <- mean((dat$y - probs)^2)
  
  tibble(AUC = as.numeric(auc_val), Brier = brier, lambda = NA)
}

```

```{r echo=FALSE, results='hide', include=FALSE, message=FALSE}
fit_logistic_ridge <- function(dat) {
  X <- as.matrix(dat[, -1])
  y <- dat$y
  
  cv.glmnet(x = X, y = y, alpha = 0, family = "binomial")
}

evaluate_ridge_model <- function(cvfit, dat) {
  X <- as.matrix(dat[, -1])
  y <- dat$y
  
  probs <- predict(cvfit, newx = X, type = "response", s = "lambda.min")
  
  auc_val <- auc(y, probs)
  brier <- mean((y - probs)^2)
  
  tibble(
    AUC = as.numeric(auc_val),
    Brier = brier,
    lambda = cvfit$lambda.min
  )
}

```

```{r echo=FALSE, results='hide', include=FALSE, message=FALSE, warning=FALSE}
set.seed(500)

EPV_vals <- c(5, 10, 50)
event_frac <- 0.5    # FIXED

P_vals <- c(4, 8, 12)
rho_vals <- c(0, 0.5)
beta_patterns <- c("equal", "strong", "noise", "halfnoise")

B <- 10  # number of Monte Carlo repetitions

all_results <- list()
iter <- 1

for (EPV in EPV_vals) {
  for (P in P_vals) {
    for (rho in rho_vals) {
      for (bp in beta_patterns) {
        for (b in 1:B) {
          
          # Determine total sample size
          N <- ceiling((EPV * P) / event_frac)
          
          # Simulate dataset
          dat <- simulate_logit(N, P, rho, bp)
          
          ### MLE ###
          fit_mle <- fit_logistic_mle(dat)
          eval_mle <- evaluate_mle_model(fit_mle, dat)
          eval_mle$model <- "MLE"
          
          ### Ridge ###
          fit_ridge <- fit_logistic_ridge(dat)
          eval_ridge <- evaluate_ridge_model(fit_ridge, dat)
          eval_ridge$model <- "Ridge"
          
          ### Store combined ###
          combined <- bind_rows(eval_mle, eval_ridge) %>%
            mutate(
              EPV = EPV,
              P = P,
              rho = rho,
              beta_pattern = bp,
              sim = b
            )
          
          all_results[[iter]] <- combined
          iter <- iter + 1
        }
      }
    }
  }
}

results_df <- bind_rows(all_results)

```

## Main Findings

1. When EPV is small (EPV=5), Ridge performs better overall than MLE, showing:
* higher or similar AUC
* lower Brier scores
* much lower variance
* more realistic estimates when predictors are correlated

2. At EPV=10, both methods perform comparably,
* Although, Ridge remains slightly more stable, especially in the presence of noisy predictors.

3. At EPV=50, MLE and Ridge are almost identical, because shrinkage is no longer needed.

4. Ridge's adaptive penalty $(\lambda)$ adjusts appropriately:
* strong shrinkage at low EPV
* minimal shrinkage at high EPV
